<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<!-- saved from url=(0026)https://sjtuytc.github.io/ -->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="keywords" content="Yizhou Li, Tokyo Tech">
<meta name="description" content="Yizhou Li&#39;s homepage">
<link rel="stylesheet" href="./resources/jemdoc.css" type="text/css">
<title>Yizhou Li's Homepage</title>
<script async="" src="./resources/analytics.js.ダウンロード"></script><script type="text/javascript" async="" src="./resources/ga.js.ダウンロード"></script><script async="" src="./resources/analytics.js(1).ダウンロード"></script><script type="text/javascript" async="" src="./resources/ga.js(1).ダウンロード"></script><script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-39824124-1']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>
</head>
<body data-new-gr-c-s-check-loaded="14.1031.0" data-gr-ext-installed="">

<div id="layout-content" style="margin-top:25px">

<table>
	<tbody>
		<tr>
			<td width="570">
				<div id="toptitle">
<!--					<h1>Yizhou Li &nbsp; <img src="./resources/name_chs_zelin.png" height="50px" style="margin-left:3px; margin-bottom:-15px"></h1><h1>-->
					<h1>Yizhou Li &nbsp; </h1><h1>

				</h1></div>

				<h3>Ph.D. Candidate</h3>
				<p>System and Control Engineering Department<br>
					Tokyo Institute of Technology
					<br>
					Email: yli [at] ok.sc.e.titech.ac.jp
					<br>
					[<a href="https://scholar.google.com/citations?hl=en&user=zcKv1JQAAAAJ&amp;hl=en">Google Scholar</a>] |
					[<a href="http://www.ok.sc.e.titech.ac.jp/">Lab</a>]
<!--					[<a href="https://www.zhihu.com/people/zhao-ytc">Zhihu</a>]-->
				</p>
				
			</td>
			<td>
				<img src="./resources/personal_image.jpg" border="0" height="250"><br>
			</td>
		</tr><tr>
	</tr></tbody>
</table>

<h2>Biography</h2>
<p>
	I started my Ph.D. life in the <a href="https://educ.titech.ac.jp/sc/eng/">SCE Department of Tokyo Tech</a> in 2021 Fall</a>.
	My supervisor is <a href="http://www.ok.sc.e.titech.ac.jp/mem/mxo/okutomi.html">Prof. Masatoshi Okutomi</a>, and I also work with <a href="http://www.ok.sc.e.titech.ac.jp/~ymonno/">Prof. Yusuke Monno</a>.
	Prior to that, I received M.E. degree in the same department of Tokyo Tech in 2021 Fall (GPA 3.50/4), and B.E. degree in CS Department from <a href="https://en.scu.edu.cn/">SCU</a> in 2019 Fall (GPA 3.57/4).
</p>
<p>My research is in computer vision and image processing. I am particularly interested in low-level vision and 3D reconstruction.</p>

<h2>Publications</h2>
<ul>
	<li>
		<a href="">TDM: Temporally-Consistent Diffusion Model for All-in-One Real-World Video Restoration</a><br>
		<b>Yizhou Li</b>, Zihua Liu, Yusuke Monno, Masatoshi Okutomi<br>
		<em>International Conference on Multimedia Modeling</em> (<b>MMM</b>), 2025<br>
		<b>Best Paper Candidates.</b><br>
	</li>
	<li>
		<a href="">Dual-Pixel Raindrop Removal</a><br>
		<b>Yizhou Li</b>, Yusuke Monno, Masatoshi Okutomi<br>
		<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (<b>TPAMI</b>),
		2024<br>
	</li>

	<li>
	<a href="">Instance-Wise MRI Reconstruction Based on Self-Supervised Implicit Neural Representation</a><br>
		Songxiao Yang, <b>Yizhou Li</b>, Masatoshi Okutomi<br>
	<em>The Annual International Conference of the IEEE Engineering in Medicine and Biology Society</em> (<b>EMBC</b>), 2024<br>
	</li>
	<li>
		<a href="">A Light-weight Universal Medical Segmentation Network for Laptops Based on Knowledge Distillation</a><br>
		Songxiao Yang, <b>Yizhou Li</b>, Ye Chen, Zhuofeng Wu, Masatoshi Okutomi<br>
		<em>Foundation Models for Medical Vision</em> (<b>CVPR Workshop</b>), 2024<br>
		<b>Meritorious Winner Award (Top 5).</b><br>
	</li>
	<li>
		<a href="">CFDNet: A Generalizable Foggy Stereo Matching Network with Contrastive Feature Distillation</a><br>
		Zihua Liu, <b>Yizhou Li</b>, Masatoshi Okutomi<br>
		<em>IEEE International Conference on Robotics and Automation</em> (<b>ICRA</b>), 2024<br>
	</li>
	<li>
		<a href="">Global Occlusion-Aware Transformer for Robust Stereo Matching</a><br>
		Zihua Liu, <b>Yizhou Li</b>, Masatoshi Okutomi<br>
		<em>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em> (<b>WACV</b>), 2024<br>
	</li>
	<li>
		<a href="http://www.ok.sc.e.titech.ac.jp/res/SIR/index.html">Dual-Pixel Raindrop Removal</a><br>
		<b>Yizhou Li</b>, Yusuke Monno, Masatoshi Okutomi<br>
		<em>British Machine Vision Conference</em> (<b>BMVC</b>),
		2022<br>
		<b>Oral Presentation, rate: 3.1% in all submissions.</b><br>
	</li>
	<li>
		<a href="http://www.ok.sc.e.titech.ac.jp/res/SIR/index.html">Single Image Deraining Network with Rain Embedding Consistency and Layered LSTM</a><br>
		<b>Yizhou Li</b>, Yusuke Monno, Masatoshi Okutomi<br>
		<em>Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em> (<b>WACV</b>), 2022<br>
	</li>

	<li>
		<a href="https://ieeexplore.ieee.org/abstract/document/9511405">Recurrent RLCN-Guided Attention Network for Single Image Deraining</a><br>
		<b>Yizhou Li</b>, Yusuke Monno, Masatoshi Okutomi<br>
		<em>The 17th International Conference on Machine Vision and Applications</em> (<b>MVA</b>), 2021<br>
	</li>

	<li>
		<a href="https://europepmc.org/article/med/33377348">Advances in the Application of Machine Learning in Maxillofacial Cysts and Tumors</a><br>
		Hongxiang Mei, Junhao Cheng, <b>Yizhou Li</b>, Huangshui Ma, Kaiwen Zhang, Yuke Shou, Yang Li<br>
		West China Journal of Stomatology, 2020<br>
	</li>


	<li>
		<a href="https://ecai2020.eu/papers/27_paper.pdf">Deep Density-aware Count Regressor</a><br>
		Zhuojun Chen, Junhao Cheng, Yuchen Yuan, Dongping Liao, <b>Yizhou Li</b>, Jiancheng Lv<br>
		<em>The 24th European Conference on Artificial Intelligence</em> (<b>ECAI</b>), 2020<br>
	</li>

	<li>
		<a href="https://ieeexplore.ieee.org/abstract/document/8857799">CLPnet: Cleft Lip and Palate Surgery Support with Deep Learning</a><br>
		<b>Yizhou Li</b>, Junhao Cheng, Hongxiang Mei, Huangshui Ma, Zhuojun Chen, Yang Li<br>
		<em>The 41st Annual International Conference of the IEEE Engineering in Medicine and Biology Society</em> (<b>EMBC</b>), 2019<br>
	</li>

</ul>

<!-- <h2>Publications (Submitted)</h2>
<ul>
	<li>
		<a href="">Dual-Pixel Raindrop Removal</a><br>
		<b>Yizhou Li</b>, Yusuke Monno, Masatoshi Okutomi<br>
		<em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> (<b>TPAMI</b>), 2024<br>
		(Status: Minor Revision, 1st Round Revision)<br>
	</li>
</ul> -->
<!-- 
<h2>Ongoing Research</h2>
<ul>
	<li>
		<a href="">High-quality Urban Neural Radiance Fields with Monocular Video</a><br>
		<b>Yizhou Li</b>, Yusuke Monno, Zijie Jiang, Zihua Liu, Masatoshi Okutomi<br>
		<ul>
			<li>
				Contents: To achieve high-quality view synthesis in urban scenes, we require rich training data from various viewpoints, such as BlockNeRF and SUDS. However, collecting such data is cost-prohibitive. We aim to utilize low-cost monocular drive recorder videos to reconstruct high-quality 3D scenes using Neural Radiance Fields (NeRF). Challenges arise due to limited viewing angles, a restricted number of viewpoints for specific objects, and limited distances to all the objects of interest. To address these challenges and improve results, we are considering to introduce noval techniques.
			</li>
		</ul>
	</li>
	<li>
		<a href="">Self-supervised Depth-from-Defocus and Multi-focus Image Fusion using Differentiable Rendering</a><br>
		<b>Yizhou Li</b>, Okutomi Masatoshi, Rafal Mantiuk<br>
	</li>
</ul> -->

<h2>Awards</h2>
<table style="border-spacing:2px" width="100%">
	
	<tbody><tr><td> <b>Meritorious Winner Award, CVPR 2024: Segment Anything In Medical Images On Laptop </b> </td> <td> 2024 </td></tr>
		<tr><td> <b>Miura Award, The Japan Society of Mechanical Engineers </b> </td> <td> 2022 </td></tr>
		<tr><td> <b>Outstanding Graguates, SCU </b> </td> <td> 2019 </td></tr>
		<tr><td> <b>Microsoft Imagine Cup: National 2nd Prize </b></td> <td> 2018 </td></tr>
		<tr><td> <b>Undergratuate Training Program for Innovation and Entrepreneurship: National Project </b></td> <td> 2018 </td></tr>
		<tr><td> <b>Blue Bridge Cup Software Development Competition: National 3rd Prize </b></td> <td> 2017 </td></tr>
	</tbody>
</table>

<h2>Scholarships</h2>
<table style="border-spacing:2px" width="100%">

	<tbody><tr><td> <b>JST Support for Pioneering Research Initialized by the Next Generation </b></td> <td> Sep 2021 - Sep 2024 </td></tr>
		<tr><td> <b>Moritani Scholarship Foundation Scholarship </b> </td> <td> April 2021- Sep 2021 </td></tr>
	</tbody>
</table>


<h2>Experiences</h2>
<ul>
	<li>
		Visiting Student | <a href="https://www.cst.cam.ac.uk/">University of Cambridge</a> | July 2023 – Oct 2023<br>
		Advisor: <a href="https://www.cl.cam.ac.uk/~rkm38/">Rafal Mantiuk</a><br>
		Topic: Self-supervised Depth-from-Defocus and Multi-focus Image Fusion using Differentiable Rendering<br>
		<!-- (Status: research becomes collaborative project with my current supervisor)<br> -->
	</li>
	<li>
		Internship | <a href="https://www.oppo.com/jp/events/jrc/">OPPO Japan Research Center</a> | March 2022 – November 2022<br>
		Advisor: <a href="https://jp.linkedin.com/in/tatsuya-baba-85192186">Tatsuya Baba</a>, <a href="https://www.linkedin.com/in/hiroyuki-uchiyama-5b7451129/?originalSubdomain=jp">Hiroyuki Uchiyama</a>, <a href="https://jp.linkedin.com/in/ahmedboudissa/en">Ahmed Boudissa</a><br>
		Topic: Generalized Stereo Vision on Smartphone<br>
	</li>
	<li>
		Internship | <a href="https://ir.baidu.com/">Baidu, Inc.</a> | Jan 2018 – May 2018<br>
		Position: R&D of Department of Natural Language and Processing<br>
		Topic: Baidu Map LBS Chatbot<br>
	</li>
</ul>

<h2>Academic Service</h2>
<table id="Service" border="0" width="100%">
	<tbody>
		<tr>
			<td>Reviewer for the IEEE Transactions on Pattern Analysis and Machine Intelligence </td>
			<td>2023</td>
		</tr>
		<tr>
			<td>Reviewer for the Pattern Recognition </td>
			<td>2023</td>
		</tr>
		<tr>
			<td>Reviewer for the IEEE Transactions on Neural Networks and Learning Systems </td>
			<td>2022</td>
		</tr>
		<tr>
			<td>Reviewer for the IEEE Transactions on Pattern Analysis and Machine Intelligence </td>
			<td>2022</td>
		</tr>
	</tbody>
</table>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-88615920-1', 'auto');
  ga('send', 'pageview');

</script>

</div>

<grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration>
</body></html>
